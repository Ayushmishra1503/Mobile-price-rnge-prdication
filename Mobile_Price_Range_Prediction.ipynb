{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "bqVZBxrvPD8n",
        "Uw0GT_uHrHcb",
        "qTEiGUKorHcc",
        "mwHQVoIQrIPu",
        "Kwy5hImkrIPu",
        "0sjxqkK0rIPv",
        "3vIdo55JHL50",
        "IlhlbgNBHL51",
        "DEMFeE18HO92",
        "MR3eS7xTHO93",
        "JmI1Y4YbHO93",
        "B4XxyvTzHSAZ",
        "Mal1wQlpHSAZ",
        "BgP_e3WlHSAZ",
        "hTF1e9PwfKqa",
        "1xrBLU6IfKqa",
        "yiLHMWFSfKqb",
        "YPEH6qLeZNRQ",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "0VL2QIwzOEmh",
        "69qjvWOxOEmh",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "peAK6Cc_HQeo",
        "khncImPpHcol",
        "wTe8K5rdYGV1",
        "yyzM232tatvC",
        "OMC09DFCbrEm",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushmishra1503/Mobile-price-rnge-prdication/blob/main/Mobile_Price_Range_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Mobile price range prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "ILM16xV1PD8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "bqVZBxrvPD8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "ewCWewEWstjz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CIYTlgLP-pdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the competitive mobile phone market companies want to understand sales data of mobile phones and factors which drive the prices. The objective is to find out some relation between features of a mobile phone(eg:- RAM, Internal Memory, etc) and its selling price. In this problem, we do not have to predict the actual price but a price range indicating how high the price is."
      ],
      "metadata": {
        "id": "K-CY5gUTcFc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Description\n",
        "Battery_power - Total energy a battery can store in one time measured in mAh\n",
        "\n",
        "Blue - Has bluetooth or not\n",
        "\n",
        "Clock_speed - speed at which microprocessor executes instructions\n",
        "\n",
        "Dual_sim - Has dual sim support or not\n",
        "\n",
        "Fc - Front Camera mega pixels\n",
        "\n",
        "Four_g - Has 4G or not\n",
        "\n",
        "Int_memory - Internal Memory in Gigabytes\n",
        "\n",
        "M_dep - Mobile Depth in cm\n",
        "\n",
        "Mobile_wt - Weight of mobile phone\n",
        "\n",
        "N_cores - Number of cores of processor\n",
        "\n",
        "Pc - Primary Camera mega pixels\n",
        "\n",
        "Px_heig Px_height - Pixel Resolution Height\n",
        "\n",
        "Px_width - Pixel Resolution Width\n",
        "\n",
        "Ram - Random Access Memory in Mega Bytes\n",
        "\n",
        "Sc_h - Screen Height of mobile in cm\n",
        "\n",
        "Sc_w - Screen Width of mobile in cm\n",
        "\n",
        "Talk_time - longest time that a single battery charge will last\n",
        "\n",
        "Three_g - Has 3G or not\n",
        "\n",
        "Touch_screen - Has touch screen or not\n",
        "\n",
        "Wifi - Has wifi or not\n",
        "\n",
        "Price_range - This is the target variable with value of 0(low cost), 1 (medium cost), 2(high cost) and 3(very high cost)."
      ],
      "metadata": {
        "id": "ygo_ax80cNDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Import Libraries\n",
        "import missingno as msno\n",
        "import math\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/data_mobile_price_range.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First \n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns \n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The tail () function is used to get the last n rows. \n",
        "df.tail()"
      ],
      "metadata": {
        "id": "x8vbiMSH2mak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing val# Visualizing the missing values\n",
        "msno.bar(df,figsize=(10,5), color=\"orange\")"
      ],
      "metadata": {
        "id": "2ma45uhy5JWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the help of bar graph we find mobile phones are in 4 price ranges. The number of elements is almost similar.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "n87BaXA_42-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The nunique () method returns the number of unique values for each column\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "IuhOPQV76F5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns.tolist():\n",
        "  print(\"The Unique Values of', i, 'are:\",df[i].unique())"
      ],
      "metadata": {
        "id": "d1TUAaNI6O0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a). Handling null values from CompetitionDistance feature."
      ],
      "metadata": {
        "id": "strTOIxQ6znu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df.isnull()"
      ],
      "metadata": {
        "id": "r-t4osyK61dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Total phones with sc_w = 0\n",
        "print(len(df[df.sc_w == 0]))\n",
        "# Total phones with px_height = 0\n",
        "print(len(df[df.px_height == 0]))"
      ],
      "metadata": {
        "id": "kvLVN_t-bpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#where ther is sc_W and px_height is zero ,assigning mean values\n",
        "df['sc_w'][df[df.sc_w == 0].index] = df.sc_w.mean()\n",
        "df['px_height'][df[df.px_height == 0].index] = df.px_height.mean()"
      ],
      "metadata": {
        "id": "FXRLnSJE7jPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#checking whether there is duplicates or not\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "qjHfcdcz7jJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values in this dataset.\n"
      ],
      "metadata": {
        "id": "xSzobCzO7wqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to visualize each chart properly we will do data visualization in a structured way following ' UBM ' rule:\n",
        "\n",
        "Univariate Analysis\n",
        "\n",
        "Bivariate Analysis\n",
        "\n",
        "Multivariate Analysi"
      ],
      "metadata": {
        "id": "t37oizHz8GcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Univariate Analysis\n",
        "\n",
        "Categorical Data"
      ],
      "metadata": {
        "id": "qog72vR28LUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Pie Chart  (Univariate)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#classes\n",
        "sns.set()\n",
        "price_plot=df['price_range'].value_counts().plot(kind='bar')\n",
        "plt.xlabel('price_range')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the help of bar graph we find mobile phones are in 4 price ranges. The number of elements is almost similar."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from this chart. there are mobile phones in 4 price ranges. The number of elements is almost similar."
      ],
      "metadata": {
        "id": "Atv4b3MijMBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see price_range are more for densely located stores.So, price range can continue providing exciting offers and services to attract customers to compete the market."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pointplot(y=\"int_memory\", x=\"price_range\", data=df)"
      ],
      "metadata": {
        "id": "FumjS6_3h9w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "% of Phones which support 3G"
      ],
      "metadata": {
        "id": "14KNsX0lk8sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Battery (Univariate)"
      ],
      "metadata": {
        "id": "xJivPyE8q_2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "sns.set(rc={'figure.figsize':(5,5)})\n",
        "ax=sns.displot(df[\"battery_power\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NTkxmIkWq_20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "azX1PEddq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows how the battery mAh is spread. there is a gradual increase as the price range increases"
      ],
      "metadata": {
        "id": "QTXVnJ-fq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Account Length with Churn Wise (Univariate)"
      ],
      "metadata": {
        "id": "Of3PJYNbrGff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chart3 Bluetooth\n",
        "\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(10,5))\n",
        "sns.barplot(data=df,x='blue',y='price_range',ax=ax)"
      ],
      "metadata": {
        "id": "tnFltgWlrGfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the help of this graph we predict that half the devices have Bluetooth, and half donâ€™t."
      ],
      "metadata": {
        "id": "_Lbuwy2Flar-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "w8bcvZxarGfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chart 5\n"
      ],
      "metadata": {
        "id": "xnMZvg2orIPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Visualizing pixel width\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "sns.kdeplot(data=df, x='px_width', hue='price_range', ax=axs[0])\n",
        "sns.boxplot(data=df, x='price_range', y='px_width', ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BIKEKoqErIPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "mwHQVoIQrIPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is not a continuous increase in pixel width as we move from Low cost to Very high cost. Mobiles with 'Medium cost' and 'High cost' has almost equal pixel width. so we can say that it would be a driving factor in deciding price_range."
      ],
      "metadata": {
        "id": "QgkYKhL2lzCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 "
      ],
      "metadata": {
        "id": "y7avNpR9HL5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
        "sns.kdeplot(data=df, x='px_height', hue='price_range', ax=axs[0])\n",
        "sns.boxplot(data=df, x='price_range', y='px_height', ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XupyxJMCHL50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Z5yPuW4iHL50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pixel height is almost similar as we move from Low cost to Very high cost.little variation in pixel_height"
      ],
      "metadata": {
        "id": "32zUPRSol-BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 front camera mega pixels(Bivariate)"
      ],
      "metadata": {
        "id": "oQ-pHCWMHO92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.plot(x='price_range',y='fc',kind='scatter')\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "Eb_s6DRSHO92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "DEMFeE18HO92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.\n",
        "\n",
        "Thus, I have used the scatter plot to depict the relationship between evening, day &n night calls , minutes and charge.\n",
        "\n",
        "The bar graph is used to compare the items between different groups over time. Bar graphs are used to measure the changes over a period of time. When the changes are larger, a bar graph is the best option to represent the data.\n",
        "\n",
        "Thus, I have used the bar plot to show the evening, night and day manipulated data to depict meaningful insights."
      ],
      "metadata": {
        "id": "zyFFR4BgHO92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "MR3eS7xTHO93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This features distribution is almost similar along all the price ranges variable, it may not be helpful in making predictions"
      ],
      "metadata": {
        "id": "GMQY2lG3mJU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8  (Bivariate)"
      ],
      "metadata": {
        "id": "XGShqbS2HSAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's drop sc_h and s_w\n",
        "\n",
        "df.drop(['sc_h', 'sc_w'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "_11-9h1oruut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_features = [ 'four_g', 'three_g']"
      ],
      "metadata": {
        "id": "8iyu1BInrvyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for col in binary_features:\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (15, 6))\n",
        "\n",
        "  df[col].value_counts().plot.pie (autopct='%1.1f%%', ax = ax1, shadow=True, labeldistance=None)\n",
        "  ax1.set_title('Distribution by price range')\n",
        "  ax1.legend(['Support', 'Does not Support'])\n",
        "  sns.countplot(x = col, hue = 'price_range', data = df, ax = ax2, color = 'pink')\n",
        "  ax2.set_title('Distribution by price range')\n",
        "  ax2.set_xlabel(col)\n",
        "  ax2.legend(['Low Cost', 'Medium Cost', 'High Cost', 'Very High Cost'])\n",
        "  ax2.set_xticklabels(['Does not Support', 'Support'])\n",
        "     "
      ],
      "metadata": {
        "id": "-6GTfuViHSAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "B4XxyvTzHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the help of pie chart we find feature 'three_g' play an important feature in prediction"
      ],
      "metadata": {
        "id": "OhGx3apQmse4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "Mal1wQlpHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose the pie chart as it represents the distribution of price range ."
      ],
      "metadata": {
        "id": "Jer-8G9xmdjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Multivariate Analysis"
      ],
      "metadata": {
        "id": "I3aNiIh9fKqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "correlation = df.corr()\n",
        "plt.figure(figsize = [10, 15])\n",
        "sns.heatmap(correlation, cmap = 'coolwarm', annot = True)"
      ],
      "metadata": {
        "id": "WPVINT_UfKqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "hTF1e9PwfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram is a popular graphing tool. It is used to summarize discrete or continuous data that are measured on an interval scale. It is often used to illustrate the major features of the distribution of the data in a convenient form. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "XOYaZpKpfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "1xrBLU6IfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAM and price_range shows high correlation which is a good sign, it signifies that RAM will play major deciding factor in estimating the price range.\n",
        "\n",
        "There is some collinearity in feature pairs ('pc', 'fc') and ('px_width', 'px_height'). Both correlations are justified since there are good chances that if front camera of a phone is good, the back camera would also be good.\n",
        "\n",
        "Also, if px_height increases, pixel width also increases, that means the overall pixels in the screen. We can replace these two features with one feature. Front Camera megapixels and Primary camera megapixels are different entities despite of showing colinearity. So we'll be keeping them as they are."
      ],
      "metadata": {
        "id": "rrwjuBlHfKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "yiLHMWFSfKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just a histogram and box plot cannot define business impact. It's done just to see the distribution of the column data over the dataset."
      ],
      "metadata": {
        "id": "7cTtRDwufKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Correlation Heatmap (multivariate)"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  defining new variable for pixels\n",
        "\n",
        "df['pixels'] = df['px_height']*df['px_width']\n",
        "# Dropping px_height and px_width\n",
        "\n",
        "df.drop(['px_height', 'px_width'], axis = 1, inplace = True)\n",
        "# Checking for multi-collinearity\n",
        "\n",
        "correlation = df.corr()\n",
        "plt.figure(figsize = [20, 15])\n",
        "sns.heatmap(correlation, cmap = 'coolwarm', annot = True)"
      ],
      "metadata": {
        "id": "1kH4dG6ysTuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram is a popular graphing tool. It is used to summarize discrete or continuous data that are measured on an interval scale. It is often used to illustrate the major features of the distribution of the data in a convenient form. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "WXw7OvvanQkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "df['fc'].hist(alpha=0.5,color='blue',label='Front camera')\n",
        "df['pc'].hist(alpha=0.5,color='red',label='Primary camera')\n",
        "plt.legend()\n",
        "plt.xlabel('MegaPixels')"
      ],
      "metadata": {
        "id": "9CnHuaq_noey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No of Phones vs Camera megapixels of front and primary camera"
      ],
      "metadata": {
        "id": "PVAQFKVnoCt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have different statistical tests for different scenarios:\n",
        "\n",
        "1.Single categorical feature -> One proportion test\n",
        "\n",
        "2.Two categorical features -> Chi squared test\n",
        "\n",
        "3.More than two category in categorical features -> ANOVA test\n",
        "\n",
        "4.One numerical and one categorical(=2 categories) feature-> ANOVA test\n",
        "\n",
        "5.One numerical feature -> T-test"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9C3DDHpRUM7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N = 30\n",
        "\n",
        "Alternate Hypothesis : N < 30\n",
        "\n",
        "Test Type: Left Tailed Test\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"price_range\"]\n",
        "second_sample = df[\"battery_power\"]\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical test provides a mechanism for making quantitative decisions about a process or processes. with the help of pearson corelation we found that mechanism is rejected null hypothesis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "Customers not churning having customer care calls average of at most 4."
      ],
      "metadata": {
        "id": "jLrtcv83OEmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "5djnGilGOEmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : N = 4\n",
        "\n",
        "Alternate Hypothesis : N > 4\n",
        "\n",
        "Test Type : Right Tailed Test\n",
        "\n"
      ],
      "metadata": {
        "id": "ps8ZluDUOEmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "DADo_qtwOEmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"n_cores\"]\n",
        "second_sample = df[\"mobile_wt\"]\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "0v4QJexjOEmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "0VL2QIwzOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ],
      "metadata": {
        "id": "9EZtE-psOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "69qjvWOxOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical test provides a mechanism for making quantitative decisions about a process or processes. with the help of pearson correlation we found that mechanism is accept null hypothesis."
      ],
      "metadata": {
        "id": "1WtUDDNROEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "Customers churning have total average call minutes including day, evening, night and international calls is 215."
      ],
      "metadata": {
        "id": "FqZJ_xAgOH3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "pE65w-N1OH3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : mean = 215\n",
        "\n",
        "Alternate Hypothesise : mean != 215\n",
        "\n",
        "Type of Test : Two Tailed test"
      ],
      "metadata": {
        "id": "9QVaMT2vOH3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "OISwWHvhOH3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "first_sample = df[\"touch_screen\"].head(2500)\n",
        "second_sample = df[\"price_range\"].head(2500)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "yOMTknz0OH3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "rheC10M4OH3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ],
      "metadata": {
        "id": "Obd1DMgROH3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "-CtoKDX7OH3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical test provides a mechanism for making quantitative decisions about a process or processes. with the help of pearson correlation we found that mechanism is rejected null hypothesis."
      ],
      "metadata": {
        "id": "6WsGNfywppYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for outliers using box plot\n",
        "plt.figure(figsize=(25,10))\n",
        "for index,item in enumerate([i for i in df.describe().columns.to_list()] ):\n",
        "  plt.subplot(5,5,index+1)\n",
        "  sns.boxplot(df[item])\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "phr1yHPPxmUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdEiDCi_kAFM"
      },
      "source": [
        "There are no much outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defining X and y\n",
        "\n",
        "X = df.drop(['price_range'], axis = 1)\n",
        "y = df['price_range']"
      ],
      "metadata": {
        "id": "0F1sA17Ix7mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Y7_EWhiNyGbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly we have \"StoreType\", \"Assortment\", \"PromoInterval\" \"SateHoliday\" as \"object\". To feed them as an input of our Machine Learning algorithm, we need to use some encoding technique to make dtype of these column as \"integer\"."
      ],
      "metadata": {
        "id": "mJ2o1RtaqNgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the unique counts of object dype column which is essential to determine the type of encoding to use in various column\n",
        "for unique in df:\n",
        "  print(f\"{unique}: \")\n",
        "  print(f\"The unique values are: {df[unique].unique()}\")\n",
        "  print(f\"Total number of unique values are: {df[unique].nunique()}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "eePr_q3_yTqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Ordinal encoding: Used when the features are ordinal in nature and have some rank between them.\n",
        "\n",
        "2.Nominal encoding: Used when the features have equal weightage and are nominal in nature.\n",
        "\n",
        "As our all the categorical columns are nominal in nature(do not have any rank or order) so will use One-Hot Encoding (Type of Nominal encoding) in our senario:"
      ],
      "metadata": {
        "id": "ok57fzWeqR6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "## We don't have textual data in any of the feature so it is not needed for our project"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of dataset\n",
        "df.shape"
      ],
      "metadata": {
        "id": "PLWgA-MabkJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "for i in df.columns:\n",
        "  print(f\"The number of unique counts in feature {i} is: {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "MXJ5JnzHNhjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining X and y\n",
        "\n",
        "X = df.drop(['price_range'], axis = 1)\n",
        "y = df['price_range']"
      ],
      "metadata": {
        "id": "YZvWZm3kN1ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X.shape"
      ],
      "metadata": {
        "id": "KnW1i_m2N5cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y.shape"
      ],
      "metadata": {
        "id": "0hyK_O-iOAD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scaling values of X\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "YrCtgp_DOD9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting dataset into train and test sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.20, random_state = 42)"
      ],
      "metadata": {
        "id": "AuM9XIY7OJTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "MDZqAAPaONcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "8HWNtqCwOTnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have used minmax Scaler of sklearn library to scale our data. This is important for us, as features on different scales can lead to poor performance or slow convergence. Standardizing the features also makes it easier to compare different features or observe the effect of a feature on the target variable(\"Sales\") by comparing the magnitude of its coefficient. Additionally, we are going to apply linear regression model for which having normally distributed data is the statistical assumption of the model, which standardization can help enforce"
      ],
      "metadata": {
        "id": "-_-SaQemdYeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per my knowledge, for this dataset dimensionality reduction is not required.\n",
        "\n",
        "Essentially where high dimensions are a problem or where it is a particular point in the algorithm to dimension reduction.\n",
        "\n",
        "Hard rules are hard to state, other than â€œafter you have tried it, did it improve mattersâ€, which isnâ€™t always the most useful guidance.\n",
        "\n",
        "Instead, looking at why we might want to do this we can get a bit of insight. Admittedly some of the following might blur together a bit at the edges but the aim is to give a flavour.\n",
        "\n",
        "1. Our data are too big. 4 million rows. 50,000 columnsâ€¦ is there a lot of redundancy there? Building a model on this could be very expensive. Even relatively simple dimension reduction techniques like PCA can capture almost all of the information in a fraction of the memory if there are strong relationships (that can be linearly approximated) in the data.\n",
        "\n",
        "2. We are over-fitting. If you build a model with tens of thousands of degrees of freedom but donâ€™t have a lot of examples you can easily overfit. Dimension reduction is one way of handling this, though often not the the best\n",
        "\n",
        "3. We want to bring in external data. OK, this is a bit different but worth a note. In applications like word2vec we want to build a classifier using an embedding. We may want to classify some text into different categories but with only a limited number of examples. The complexity of free text is vast but a low dimension embedding is much smaller and will not overfit so badly in a classifier. Building a low dimensional embedding on external text, applying it to the text to be classified then building a classifier is using dimension reduction to bring in external data.\n",
        "\n",
        "4. We suffer from the curse of dimesnionality. Consider something like a nearest neighbour search. As the number of dimensions gets large we see some unwanted behaviour, especially if we are looking at things like euclidean distances. Projecting your data to a lower dimensional space for nearest neighbour, clustering or outlier detection can be both more robust and more meaningful.\n",
        "\n",
        "5. Some tools are all about this. Collaborative filtering through matrix factorisation is an example. Can we approximately describe behaviour as a linear combination of a smaller number of preferences/behaviours?"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "# DImensionality Reduction (If needed)\n",
        "## Not needed for now"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data splitting"
      ],
      "metadata": {
        "id": "6LAEDdjQQV5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=0)\n",
        "columns=X_train.columns\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "sTviHaqvQbsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_exp = X_train\n",
        "df_exp"
      ],
      "metadata": {
        "id": "apkUNgJBQhmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R5tQWErErYOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - **Implementing Logistic Regression**"
      ],
      "metadata": {
        "id": "peAK6Cc_HQeo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbsRRWRfEQtK"
      },
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF36F8oQEVIp"
      },
      "source": [
        "# Applying logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "\n",
        "y_pred_test = lr.predict(X_test)\n",
        "y_pred_train = lr.predict(X_train)\n",
        "# Evaluation metrics for test"
      ],
      "metadata": {
        "id": "-DFulmNoTTgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(fit_intercept=True, max_iter=1000)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "pw6EiFQDW57W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Classification report for Logistic Regression (Test set)= ')\n",
        "print(classification_report(y_pred_test, y_test))"
      ],
      "metadata": {
        "id": "dv_M2GI4TZce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.coef_"
      ],
      "metadata": {
        "id": "pyUxHVgyWpRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clf.intercept_"
      ],
      "metadata": {
        "id": "vXTNRdBJXFRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "khncImPpHcol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the"
      ],
      "metadata": {
        "id": "EBND2ZEvTv-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation metrics for train\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('Classification report for Logistic Regression (Train set)= ')\n",
        "print( classification_report(y_pred_train, y_train))"
      ],
      "metadata": {
        "id": "wKrKeoTwUCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "wTe8K5rdYGV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import cross_validate"
      ],
      "metadata": {
        "id": "els-hXilXSBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic = LogisticRegression()"
      ],
      "metadata": {
        "id": "PsLDo_2jXd5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scoring = ['accuracy']\n",
        "scores = cross_validate(logistic,X_train, y_train, scoring = scoring, cv = 5, return_train_score=True,return_estimator=True,verbose = 10)"
      ],
      "metadata": {
        "id": "iEnLInA-YkM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores['train_accuracy']"
      ],
      "metadata": {
        "id": "7xN29ff-YnWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores['test_accuracy']"
      ],
      "metadata": {
        "id": "5mQiFqavYrYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in scores['estimator']:\n",
        "    print(model.coef_)"
      ],
      "metadata": {
        "id": "BlPQGbhzY-_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "OMC09DFCbrEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning is an iterative process.You will face choices about predictive variables to use, what types of models to use,what arguments to supply those models, etc. We make these choices in a data-driven way by measuring model quality of various alternatives. You've already learned to use train_test_split to split the data, so you can measure model quality on the test data. Cross-validation extends this approach to model scoring (or \"model validation.\") Compared to train_test_split, cross-validation gives you a more reliable measure of your model's quality, though it takes longer to run."
      ],
      "metadata": {
        "id": "HThWwJjVrrpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - **Implementing Random Forest Classifier**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Create an instance of the RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "ULgBUAmTpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNzv3F70d3Fb"
      },
      "source": [
        "# Calculating accuracy on train and test\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# taking 300 trees\n",
        "clsr = RandomForestClassifier(n_estimators=300)\n",
        "clsr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = clsr.predict(X_test)\n",
        "test_score= accuracy_score(y_test, y_pred)\n",
        "test_score"
      ],
      "metadata": {
        "id": "yxFOpYsyaBxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = clsr.predict(X_train)\n",
        "train_score = accuracy_score(y_train, y_pred_train)\n",
        "train_score"
      ],
      "metadata": {
        "id": "WPELmkpsaHVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "l1Qvbt4-aNh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XYewGD-GaVqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_importance = pd.DataFrame({'Feature':X.columns,\n",
        "                                   'Score':clsr.feature_importances_}).sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "feature_importance.head()"
      ],
      "metadata": {
        "id": "-Ex3i4bSaiYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "ax = sns.barplot(x=feature_importance['Score'], y=feature_importance['Feature'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tz10GA7CapAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used Random Forest algorithm to create the model. As I got there is overfitting seen.\n",
        "\n",
        "We have used different combinations of parameters to get the best value of r2 score and least MAPE for our case. The best combination was found out to be {'max_depth': 18, 'min_samples_leaf': 7, 'min_samples_split': 4} which resulted into the improvement in the MSE from 22% to 19% on the test set. Also MAPE is further reduced from 4% to 3% and capturing variance 1% more i.e 91% of the test dataset. At this point of time we have achieved above 95% (=96.30%) accuracy by hyperparameter tuning of Decision trees."
      ],
      "metadata": {
        "id": "ImrbALQSRWMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.  But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. \n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. \n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, i found precision of 100% and recall of 87% and f1-score of 93% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 12% and recall of 93% and f1-score of 22%. Accuracy is 88% and average percision, recall & f1_score are 56%, 90% and 57% respectively with a roc auc score of 56%.\n",
        "\n",
        "Quite improvment seen as no overfitting but the scores reduced by some percentages.\n",
        "\n",
        "For testing dataset, i found precision of 100% and recall of 88% and f1-score of 93% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 13% and recall of 90% and f1-score of 23%. Accuracy is 88% and average percision, recall & f1_score are 56%, 89% and 58% respectively with a roc auc score of 56%.\n",
        "\n",
        "Quite improvemnt seen in recall but rest scores have decreased.\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - **Implementing XgBoost Classifier**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying XGBoost\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(max_depth = 5, learning_rate = 0.1)\n",
        "xgb.fit(X_train, y_train)\n",
        "XGBClassifier(max_depth=5, objective='multi:softprob')\n",
        "# Prediction\n",
        "\n",
        "y_pred_train = xgb.predict(X_train)\n",
        "y_pred_test = xgb.predict(X_test)\n",
        "# Evaluation metrics for test\n",
        "\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print('Classification Report for XGBoost(Test set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "GLH0iDTveX9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation metrics for train\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print('Classification Report for XGBoost(Train set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "qyXZA6KrelO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cross validation\n",
        "\n",
        "grid = GridSearchCV(xgb, param_grid={'n_estimators': (10, 200), 'learning_rate': [1, 0.5, 0.1, 0.01, 0.001], 'max_depth': (5, 10),\n",
        "                                     'gamma': [1.5, 1.8], 'subsample': [0.3, 0.5, 0.8]}, cv = 5, scoring = 'accuracy', verbose = 10)\n",
        "grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "FGYjNwhIerw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "\n",
        "y_pred_train = grid.predict(X_train)\n",
        "y_pred_test = grid.predict(X_test)\n",
        "# Evaluation metrics for test\n",
        "\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print('Classification Report for tuned XGBoost(Test set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "hZd6Mrfre6Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(cf_matrix)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels([0,1,2,3])\n",
        "ax.yaxis.set_ticklabels([0,1,2,3])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q6CH3tZlfQde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation metrics for train\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print('Classification Report for tuned XGBoost(Train set)= ')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "S8e9pF93feGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used XGBoost algorithm to create the model. As I got there  good result.\n",
        "\n",
        "For training dataset, i found precision of 100% and recall of 91% and f1-score of 95% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 46% and recall of 95% and f1-score of 62%. Accuracy is 92% and average percision, recall & f1_score are 73%, 93% and 79% respectively with a roc auc score of 72%.\n",
        "\n",
        "For testing dataset, i found precision of 99% and recall of 90% and f1-score of 94% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 35% and recall of 80% and f1-score of 48%. Accuracy is 90% and average percision, recall & f1_score are 67%, 85% and 71% respectively with a roc auc score of 66%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "ctnr9FepURk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.  But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. \n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. \n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN Model**"
      ],
      "metadata": {
        "id": "fQnlaHDx4p6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "s0Mu86ii4pON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "BuvkoQri80eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "TTcsGi_k813N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,pred))"
      ],
      "metadata": {
        "id": "zMjUJw0k9NQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix=confusion_matrix(y_test,pred)\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "-0MvWLxY9RoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(matrix,annot=True)"
      ],
      "metadata": {
        "id": "nJ_pGCjS9Yn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From EDA we can see that here are mobile phones in 4 price ranges. \n",
        "2. The number of elements is almost similar.\n",
        "3. Half the devices have Bluetooth, and half donâ€™t\n",
        "4. There is a gradual increase in battery as the price range increases\n",
        "5. Ram has continuous increase with price range while moving from Low cost to Very high cost\n",
        "6. costly phones are lighter\n",
        "7. RAM, battery power, pixels played more significant role in deciding the price range of mobile phone.\n",
        "8. From all the above experiments we can conclude that logistic regression and, XGboosting with using hyperparameters we got the best results"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}